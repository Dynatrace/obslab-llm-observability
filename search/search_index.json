{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Large Language Model Observability with Dynatrace","text":"<p>Demo application for giving travel advice written in Python. Observability signals by OpenTelemetry and OpenLLMetry.</p> <p>Uses OpenAI ChatGPT to generate advice for a given destination.</p> <p></p> <p>This hands-on is also available as an on-demand webinar.</p>"},{"location":"#architecture","title":"Architecture","text":""},{"location":"#click-here-to-start-the-tutorial","title":"&gt;&gt; Click here to start the tutorial...","text":""},{"location":"cleanup/","title":"Cleanup","text":"<p>Go to https://github.com/codespaces and delete the codespace which will delete the demo environment.</p>"},{"location":"how-it-works/","title":"2. How it works","text":"<p>The user interacts with the demo app (travel advisor) on port <code>30100</code>. The app is monitored either via native OpenTelemetry.</p> <p>The user enters a destination (eg. <code>Sydney</code>):</p> <ul> <li>The application first checks the cache.<ul> <li>If a response for <code>Sydney</code> is found, the response is returned from the cache.</li> <li>If a cached response is not available, the application requests advice from the LLM (OpenAI's ChatGPT).</li> </ul> </li> <li>The response is returned and cached so that subsequent calls for the same destination (eg. <code>Sydney</code>) are served from the cache. This saves roundtrips to ChatGPT and thus <code>$</code>.</li> </ul>"},{"location":"how-it-works/#click-here-to-continue-with-the-exercise","title":"&gt;&gt; Click here to continue with the exercise","text":""},{"location":"prerequisites/","title":"4. Demo Prerequisites","text":"<p>To run this demo you will need:</p> <ul> <li>A Dynatrace SaaS account (free trial)</li> <li>An OpenAI account with credit added</li> </ul>"},{"location":"prerequisites/#why-is-a-paid-openai-account-required","title":"Why is a Paid OpenAI Account Required?","text":"<p>OpenAI / ChatGPT severely limits the ability for API access if you do not have credit. Adding a small amount of credit ($2-$3) is the best way to make this (and all other ChatGPT demos) run smoothly.</p> <p>This demo uses gpt 4o mini. We have developed, tested and demoed this repository hundreds of times and still have money left from the initial $5 credit load.</p>"},{"location":"prerequisites/#click-here-to-continue-with-the-exercise","title":"&gt;&gt; Click here to continue with the exercise","text":""},{"location":"resources/","title":"Resources","text":"<ul> <li>This repository and documentation on GitHub</li> <li>LLM Observability On-Demand Webinar (Video)</li> </ul>"},{"location":"setup/","title":"5. Setup","text":""},{"location":"setup/#create-openai-api-token","title":"Create OpenAI API Token","text":"<p>Go to https://platform.openai.com/api-keys and create a new API Key.</p>"},{"location":"setup/#format-dynatrace-url","title":"Format Dynatrace URL","text":"<p>Make a note of your Dynatrace URL, it should be in the following format:</p> <pre><code>https://ENVIRONMENT-ID.live.dynatrace.com\n</code></pre> <p>For example:</p> <pre><code>https://abc12345.live.dynatrace.com\n</code></pre>"},{"location":"setup/#create-dynatrace-token","title":"Create Dynatrace Token","text":"<p>In Dynatrace, press <code>Ctrl + k</code> and search for <code>access tokens</code>. Choose the first option.</p>"},{"location":"setup/#dt_api_token","title":"DT_API_TOKEN","text":"<p>Create an API token with these permissions:</p> <ul> <li>Ingest metrics (<code>metrics.ingest</code>)</li> <li>Ingest logs (<code>logs.ingest</code>)</li> <li>Ingest events (<code>events.ingest</code>)</li> <li>Ingest OpenTelemetry traces  (<code>openTelemetryTrace.ingest</code>)</li> <li>Read metrics (<code>metrics.read</code>)</li> <li>Write settings  (<code>settings.write</code>)</li> </ul> <p>This token will be used by the OpenTelemetry collector and k6 to send data to Dynatrace. The setup script which runs automatically when the codespace is created also uses this to configure span attribute capture rules in Dynatrace this means the relevant OpenTelemetry span attributes will automatically be stored.</p>"},{"location":"setup/#recap","title":"\ud83d\udd01 Recap","text":"<p>You should now have <code>3</code> pieces of information:</p> <ul> <li>The <code>DT_ENDPOINT</code> (eg. <code>https://abc12345.live.dynatrace</code>)</li> <li>The <code>DT_API_TOKEN</code></li> <li>The <code>OPEN_AI_TOKEN</code></li> </ul> <p>When you have these pieces of information, you can proceed.</p>"},{"location":"setup/#click-here-to-continue-with-the-exercise","title":"&gt;&gt; Click here to continue with the exercise","text":""},{"location":"standard-rag-differences/","title":"3. Standard vs. RAG Version","text":"<p>This demo is available in two flavours.</p> <p>The \"standard\" demo uses OpenAI's ChatGPT (coupled with an on-cluster Weaviate cache) to look up destination advice for any destination.</p> <p>The \"RAG\" version (available on the rag branch) will only produce destination advice for places the system has explicitly been trained on (the files in the destinations folder on the <code>rag</code> branch). Namely, <code>Bali</code> and <code>Sydney</code>.</p> <p>This is achieved by:</p> <ul> <li>Reading each file from disk when the app starts</li> <li>Sending the contents of the bali and sydney HTML pages along with each request and explicitly telling the model to only use the information provided in those documents.</li> </ul> <p>The RAG version of the demo mimicks training an LLM on an internal knowledgebase.</p>"},{"location":"standard-rag-differences/#click-here-to-continue-with-the-exercise","title":"&gt;&gt; Click here to continue with the exercise","text":""},{"location":"startup/","title":"6. Start The Demo","text":""},{"location":"startup/#time-to-fire-it-up","title":"\ud83c\udd99 Time to Fire it up","text":"<p>Choose one of the following options to start the codespace:</p>"},{"location":"startup/#launch-standard-version","title":"Launch Standard Version","text":""},{"location":"startup/#launch-rag-version","title":"Launch RAG Version","text":"<p>Leave the top section blank and provide your values in the <code>Recommended secrets</code> form.</p> <p>After the codespaces has started (in a new browser tab), the post creation script should begin. This will install everything and will take a few moments.</p> <p>When the script has completed, a success message will briefly be displayed (it is so quick you'll probably miss it) and an empty terminal window will be shown.</p> <p></p> <p> TODO: Needs new image</p> <p>You may now proceed.</p>"},{"location":"startup/#click-here-to-continue-with-the-exercise","title":"&gt;&gt; Click here to continue with the exercise","text":""},{"location":"use-demo/","title":"7. Use the Demo","text":""},{"location":"use-demo/#accessing-and-using-demo","title":"Accessing and Using Demo","text":"<p>In the codespace, switch to the <code>Ports</code> tab. Right click port <code>30100</code> and choose <code>Open in Browser</code></p> <p></p> <p>A new browser tab will open and you should see the demo.</p> <p></p>"},{"location":"use-demo/#using-llm-based-destination-search","title":"Using LLM-based Destination Search","text":"<p>Type the name of a destination (eg. <code>Vienna</code>) into the search bar and click the <code>Advise</code> button.</p>"},{"location":"use-demo/#what-happens-next","title":"What Happens Next?","text":"<ul> <li>The application will request information for your destination from OpenAI using ChatGPT 4o mini.</li> <li>A result will be returned from OpenAI</li> <li>The result is cached in the weviate vector cache</li> </ul> <p>If you search for <code>Vienna</code> again, this time, the re1sult will be served from the cache - saving you the roundtrip (time and $) to OpenAI / ChatGPT.</p>"},{"location":"use-demo/#customer-feedback","title":"Customer Feedback","text":"<p>Click the \ud83d\udc4d and / or \ud83d\udc4e buttons to indicate your satisfaction level of the result.</p> <p>Clicking these icons will log a message. This log line is then retrieved and processed using DQL in the \"User Sentiment Analysis\" section of the dashboard.</p>"},{"location":"use-demo/#click-here-to-continue-with-the-exercise","title":"&gt;&gt; Click here to continue with the exercise","text":""},{"location":"visualise-dt/","title":"Visualising Data in Dynatrace","text":""},{"location":"visualise-dt/#uploading-the-dashboards","title":"Uploading the Dashboards","text":"<p>This demo comes with several prebuilt dashboards. Do the following in Dynatrace.</p> <ul> <li>Save the contents of dynatrace/dashboards/openai/Travel-Advisor-Overview.json to your computer</li> <li>Press  <code>Ctrl + k</code> and search for <code>dashboards</code> or select the icon from the left toolbar</li> <li>Select the <code>Upload</code> button and upload the JSON file.</li> </ul> <p></p> <p></p> <p>Repeat this process for all the dashboards inside dynatrace/dashboards/*</p>"},{"location":"visualise-dt/#distributed-traces","title":"Distributed Traces","text":"<p>The application emits distributed traces which can be viewed in Dynatrace:</p> <ul> <li>Press <code>ctrl + k</code> search for <code>distributed traces</code></li> <li>Traces for <code>/api/v1/completion</code> are created for each call to either OpenAI or a call to the Weaviate cache.</li> </ul> <p>Remember that only the very first requests for a given destination will go out to OpenAI. So expect many many more cached traces than \"live\" traces.</p>"},{"location":"visualise-dt/#trace-with-openai","title":"Trace with OpenAI","text":"<p>A \"full\" call to OpenAI looks like this. Notice the long call halfway through the trace to <code>openai.chat</code>. These traces take much longer (3 seconds vs. 500ms).</p> <p></p> <p></p>"},{"location":"visualise-dt/#trace-to-weaviate-cache","title":"Trace to Weaviate Cache","text":"<p>A call which instead only hits the on-cluster Weaviate cache looks like this.</p> <p>Notice that it is much quicker.</p> <p>The response TTL (max time that a cached prompt is considered \"fresh\") is checked and if the response is \"still fresh\" (ie. <code>TTL &lt; stale time</code>) the cached value is returned.</p> <p></p> <p>Notice the cached prompt is <code>123s</code>. The max age (TTL is (by default) <code>60</code> minutes. Therefore the prompt is not outdated and thus returned to the user as valid.</p> <p></p>"},{"location":"visualise-dt/#demo-complete","title":"\ud83c\udf89 Demo Complete","text":"<p>The demo is now complete. Continue to cleanup your environment.</p>"},{"location":"visualise-dt/#cleanup-resources-to-avoid-github-charges","title":"&gt;&gt; Cleanup resources to avoid GitHub charges","text":""},{"location":"whats-next/","title":"What's Next?","text":""},{"location":"whats-next/#resources","title":"Resources","text":""}]}